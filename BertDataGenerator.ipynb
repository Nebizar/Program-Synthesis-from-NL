{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertDataGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XTJdOYVs82b",
        "outputId": "c9a787f3-f1ff-4316-e865-3f7bfbe93ae7"
      },
      "source": [
        "!pip install tensorflow_text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_text) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_text) (0.11.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.10.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.36.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.4.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.32.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.4.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.3.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (51.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.7)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLSe-bV_s-yx"
      },
      "source": [
        "from sklearn.metrics import pairwise\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "\r\n",
        "import re\r\n",
        "import tensorflow_text as text\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from naps.pipelines.read_naps import read_naps_dataset"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHFiaDP8tBZ7"
      },
      "source": [
        "preprocessor = hub.KerasLayer(\r\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\")\r\n",
        "\r\n",
        "encoder = hub.KerasLayer(\r\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\",\r\n",
        "    trainable=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B44W1lsZtLkr"
      },
      "source": [
        "def flatten(iterable):\r\n",
        "    iterator, sentinel, stack = iter(iterable), object(), []\r\n",
        "    while True:\r\n",
        "        value = next(iterator, sentinel)\r\n",
        "        if value is sentinel:\r\n",
        "            if not stack:\r\n",
        "                break\r\n",
        "            iterator = stack.pop()\r\n",
        "        elif isinstance(value, str):\r\n",
        "            yield str(value)\r\n",
        "        else:\r\n",
        "            try:\r\n",
        "                new_iterator = iter(value)\r\n",
        "            except TypeError:\r\n",
        "                yield value\r\n",
        "            else:\r\n",
        "                stack.append(iterator)\r\n",
        "                iterator = new_iterator\r\n",
        "\r\n",
        "\r\n",
        "# trainA, trainB, test = read_naps_dataset()\r\n",
        "# for name, ds in zip((\"trainA\", \"trainB\", \"test\"), read_naps_dataset()):\r\n",
        "#     print(\"DATASET %s\" % name)\r\n",
        "def get_unique_tokens():\r\n",
        "    ds, _, test = read_naps_dataset()\r\n",
        "    tokens_total = []\r\n",
        "    with ds:\r\n",
        "        \r\n",
        "        for d in ds:\r\n",
        "            if \"is_partial\" in d and d[\"is_partial\"]:\r\n",
        "                continue\r\n",
        "            #print(' '.join(d[\"text\"]))\r\n",
        "            #uast_pprint.pprint(d[\"code_tree\"])\r\n",
        "            \r\n",
        "            tokens = list(set(list(make_the_tree_good(d[\"code_tree\"][\"funcs\"]))))\r\n",
        "            tokens_total += tokens\r\n",
        "\r\n",
        "    with test:\r\n",
        "        \r\n",
        "        for d in test:\r\n",
        "            if \"is_partial\" in d and d[\"is_partial\"]:\r\n",
        "                continue\r\n",
        "            #print(' '.join(d[\"text\"]))\r\n",
        "            #uast_pprint.pprint(d[\"code_tree\"])\r\n",
        "            \r\n",
        "            tokens = list(set(list(make_the_tree_good(d[\"code_tree\"][\"funcs\"]))))\r\n",
        "            tokens_total += tokens\r\n",
        "\r\n",
        "    tokens_total = list(set(tokens_total))\r\n",
        "    print(tokens_total) \r\n",
        "    print(len(tokens_total)) #1850\r\n",
        "\r\n",
        "    with open('tokens.txt', 'w') as f:\r\n",
        "        for token in tokens_total:\r\n",
        "            f.write(\"%s\\n\" % token)\r\n",
        "\r\n",
        "def get_tokens():\r\n",
        "    with open('tokens.txt', 'r') as f:\r\n",
        "        tokens = f.read().splitlines()\r\n",
        "        return tokens\r\n",
        "\r\n",
        "def generate_output(tokens, unique):\r\n",
        "    values = np.zeros((len(tokens), len(unique)))\r\n",
        "    for i in range(len(tokens)):\r\n",
        "        index = unique.index(str(tokens[i]))\r\n",
        "        values[i][index] = 1\r\n",
        "    return values\r\n",
        "\r\n",
        "def generate_tokens(output, unique):\r\n",
        "    output = np.array(output)\r\n",
        "    tokens = []\r\n",
        "    for out in output:\r\n",
        "        idx = np.argmax(out)\r\n",
        "        tokens.append(unique[idx])\r\n",
        "    return tokens\r\n",
        "\r\n",
        "def even_embeddings(embed, n):\r\n",
        "    if len(embed) < n:\r\n",
        "        diff =  n - len(embed)\r\n",
        "        embed = np.vstack([embed, np.zeros((diff,len(embed[0])))])\r\n",
        "    return embed\r\n",
        "\r\n",
        "def even_tokens(tokens, n):\r\n",
        "    tokens = np.concatenate((['START'],tokens,['END'] ))\r\n",
        "    if len(tokens) < n:\r\n",
        "        diff =  n - len(tokens)\r\n",
        "        tokens = np.concatenate((tokens, np.full(diff, '')))\r\n",
        "    return tokens"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQM4D9JEtX90"
      },
      "source": [
        "def get_embeddings(text, preprocessor, encoder):\r\n",
        "  sentences_dirty = text.split('.')\r\n",
        "\r\n",
        "  max_len = 0\r\n",
        "\r\n",
        "  sentences = []\r\n",
        "  for sent in sentences_dirty:\r\n",
        "    sent = re.sub('[,;-]', '', sent)\r\n",
        "    if len(sent) > 0:\r\n",
        "        sentences.append(sent)\r\n",
        "        splitted = sent.split()\r\n",
        "        if len(splitted) > max_len:\r\n",
        "            max_len = len(splitted)\r\n",
        "\r\n",
        "  encoder_inputs = preprocessor(sentences)\r\n",
        "  outputs = encoder(encoder_inputs)\r\n",
        "\r\n",
        "  x = outputs[\"sequence_output\"].numpy()\r\n",
        "\r\n",
        "  return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8ReGYqNtYpx"
      },
      "source": [
        "_, _, test = read_naps_dataset()\r\n",
        "X_train = []\r\n",
        "Y_train = []\r\n",
        "X_test = []\r\n",
        "Y_test = []\r\n",
        "\r\n",
        "Y_tokens = []\r\n",
        "Y_tokens2 = []\r\n",
        "max_embs_len = 0\r\n",
        "max_tokens_len = 0\r\n",
        "counter = 0\r\n",
        "stopper = 100\r\n",
        "stopper2 = 1\r\n",
        "# with ds:\r\n",
        "#     for d in ds:\r\n",
        "#         if \"is_partial\" in d and d[\"is_partial\"]:\r\n",
        "#             continue\r\n",
        "#         counter += 1\r\n",
        "#         masked_embs = get_embeddings(' '.join(d[\"text\"]), preprocessor, encoder)\r\n",
        "#         if len(masked_embs) > max_embs_len:\r\n",
        "#             max_embs_len = len(masked_embs)\r\n",
        "#         X_train.append(masked_embs)    \r\n",
        "#         tokens = make_the_tree_good(d[\"code_tree\"][\"funcs\"])\r\n",
        "#         #print(make_the_tree_good(d[\"code_tree\"][\"funcs\"]))\r\n",
        "#         if len(tokens) > max_tokens_len:\r\n",
        "#             max_tokens_len = len(tokens)\r\n",
        "#         Y_tokens.append(tokens)\r\n",
        "#         if counter == stopper:\r\n",
        "#             break\r\n",
        "counter = 0\r\n",
        "with test:\r\n",
        "    for t in test:\r\n",
        "        if \"is_partial\" in t and t[\"is_partial\"]:\r\n",
        "            continue\r\n",
        "        counter += 1\r\n",
        "        masked_embs = get_embeddings(' '.join(t[\"text\"]), preprocessor, encoder)\r\n",
        "        if len(masked_embs) > max_embs_len:\r\n",
        "            max_embs_len = len(masked_embs)\r\n",
        "        X_test.append(masked_embs)    \r\n",
        "        tokens = list(flatten(t[\"code_tree\"][\"funcs\"]))\r\n",
        "        #print(make_the_tree_good(d[\"code_tree\"][\"funcs\"]))\r\n",
        "        if len(tokens) > max_tokens_len:\r\n",
        "            max_tokens_len = len(tokens)\r\n",
        "        Y_tokens2.append(tokens)\r\n",
        "        if counter == stopper2:\r\n",
        "            break"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knTLN4vovgQ_"
      },
      "source": [
        "def get_BERT_x(X):\r\n",
        "  new_X = []\r\n",
        "  for ex in X:\r\n",
        "    if ex.shape[0] < 15:\r\n",
        "      diff =  15 - len(ex)\r\n",
        "      embed = np.vstack([ex, np.zeros((diff, 128, 768))])\r\n",
        "      new_X.append(embed)\r\n",
        "    elif ex.shape[0] == 15:\r\n",
        "      new_X.append(ex)\r\n",
        "    else:\r\n",
        "      continue\r\n",
        "  return np.array(new_X)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Glw-dWOvkmT"
      },
      "source": [
        "X_train = get_BERT_x(X_train)\r\n",
        "X_train = X_train.reshape(X_train.shape[0],1920,768)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMTlXKssvmtj"
      },
      "source": [
        "with open('X_train.npy', 'wb') as f:\r\n",
        "    np.save(f, X_train)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krA1lqGi0ece"
      },
      "source": [
        "X_test = get_BERT_x(X_test)\r\n",
        "X_test = X_test.reshape(X_test.shape[0],1920,768)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCb0vv_V0gd0"
      },
      "source": [
        "with open('X_test.npy', 'wb') as f:\r\n",
        "    np.save(f, X_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLdMP4MP2L3_"
      },
      "source": [
        "unique = get_tokens()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e0kvH6p2MuG"
      },
      "source": [
        "Y_train = []\r\n",
        "max = 701\r\n",
        "for ex in Y_tokens:\r\n",
        "    #print(even_tokens(ex,max_tokens_len))\r\n",
        "    Y_train.append(generate_output(even_tokens(np.array(ex),max), unique))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ9JVxPf3_fO"
      },
      "source": [
        "Y_train = np.array(Y_train)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLgjn34i3-EY"
      },
      "source": [
        "with open('Y_train.npy', 'wb') as f:\r\n",
        "    np.save(f, Y_train)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwNl8fcR4xxz"
      },
      "source": [
        "Y_test = []\r\n",
        "max = 701\r\n",
        "for ex in Y_tokens2:\r\n",
        "    #print(even_tokens(ex,max_tokens_len))\r\n",
        "    Y_test.append(generate_output(even_tokens(np.array(ex),max), unique))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jikjIjsv417O"
      },
      "source": [
        "Y_test = np.array(Y_test)\r\n",
        "with open('Y_test.npy', 'wb') as f:\r\n",
        "    np.save(f, Y_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH8ZCIJWE0Fz",
        "outputId": "49c559ac-82ef-4710-ad1b-097f2ff8c78a"
      },
      "source": [
        "Y_test.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 701, 1850)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjzEATf2E1LZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}