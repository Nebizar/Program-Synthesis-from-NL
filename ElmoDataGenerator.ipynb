{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "marine-grenada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\synth\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\synth\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\synth\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\synth\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\synth\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\synth\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\synth\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\synth\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\synth\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\synth\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\synth\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\synth\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "\n",
    "from naps.pipelines.read_naps import read_naps_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "immune-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "global elmo\n",
    "\n",
    "def init():\n",
    "    global elmo\n",
    "    elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
    "\n",
    "\n",
    "def get_words(text):\n",
    "    sentences_dirty = text.split('.')\n",
    "\n",
    "    max_len = 0\n",
    "\n",
    "    sentences = []\n",
    "    for sent in sentences_dirty:\n",
    "        sent = re.sub('[,;-]', '', sent)\n",
    "        if len(sent) > 0:\n",
    "            sentences.append(sent)\n",
    "            splitted = sent.split()\n",
    "            if len(splitted) > max_len:\n",
    "                max_len = len(splitted)\n",
    "\n",
    "    words = []\n",
    "    mask = []\n",
    "    masked_words = []\n",
    "    for sent in sentences:\n",
    "        splitted = sent.split()\n",
    "        for i in range(max_len):\n",
    "            try:\n",
    "                words.append(splitted[i])\n",
    "            except:\n",
    "                words.append('_')\n",
    "\n",
    "    for word in words:\n",
    "        if word == \"_\":\n",
    "            mask.append(False)\n",
    "        else:\n",
    "            mask.append(True)\n",
    "            masked_words.append(word)\n",
    "    return sentences, mask\n",
    "\n",
    "def get_embeddings(text):\n",
    "    global elmo\n",
    "    sentences, mask = get_words(text)\n",
    "\n",
    "    embeddings = elmo(\n",
    "        sentences,\n",
    "        signature=\"default\",\n",
    "        as_dict=True)[\"elmo\"]\n",
    "\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        sess.run(tf.compat.v1.tables_initializer())\n",
    "        x = sess.run(embeddings)\n",
    "    #print(x)\n",
    "    embs = x.reshape(-1, 1024)\n",
    "    masked_embs = embs[mask]\n",
    "\n",
    "    return np.array(masked_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "norwegian-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_the_tree_good(bad_tree):\n",
    "    # Zwracana zmienna\n",
    "    good_tree = []\n",
    "\n",
    "    # Warunek zakończenia rekurencji\n",
    "    if type(bad_tree) is not list:\n",
    "        good_tree = bad_tree\n",
    "\n",
    "    # Warunek zaistnienia rekurencji\n",
    "    else:\n",
    "        for x in bad_tree:\n",
    "            mttg = make_the_tree_good(x)\n",
    "\n",
    "            # Dobranie sposobu dołączenia do wynikowej listy\n",
    "            if type(mttg) is list:\n",
    "                good_tree.extend(mttg)\n",
    "            else:\n",
    "                good_tree.append(str(mttg))\n",
    "\n",
    "    return good_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "jewish-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_tokens():\n",
    "    ds, _, test = read_naps_dataset()\n",
    "    tokens_total = []\n",
    "    with ds:\n",
    "        \n",
    "        for d in ds:\n",
    "            if \"is_partial\" in d and d[\"is_partial\"]:\n",
    "                continue\n",
    "            \n",
    "            tokens = list(set(list(make_the_tree_good(d[\"code_tree\"][\"funcs\"]))))\n",
    "            tokens_total += tokens\n",
    "            \n",
    "    with test:\n",
    "        \n",
    "        for d in test:\n",
    "            if \"is_partial\" in d and d[\"is_partial\"]:\n",
    "                continue\n",
    "            \n",
    "            tokens = list(set(list(make_the_tree_good(d[\"code_tree\"][\"funcs\"]))))\n",
    "            tokens_total += tokens\n",
    "\n",
    "    tokens_total = list(set(tokens_total))\n",
    "    print(tokens_total) \n",
    "    print(len(tokens_total)) #1850\n",
    "\n",
    "    with open('tokens.txt', 'w') as f:\n",
    "        for token in tokens_total:\n",
    "            f.write(\"%s\\n\" % token)\n",
    "\n",
    "def get_tokens():\n",
    "    with open('tokens.txt', 'r') as f:\n",
    "        tokens = f.read().splitlines()\n",
    "        return tokens\n",
    "\n",
    "def generate_output(tokens, unique):\n",
    "    values = np.zeros((len(tokens), len(unique)))\n",
    "    for i in range(len(tokens)):\n",
    "        index = unique.index(str(tokens[i]))\n",
    "        values[i][index] = 1\n",
    "    return values\n",
    "\n",
    "def generate_tokens(output, unique):\n",
    "    output = np.array(output)\n",
    "    tokens = []\n",
    "    for out in output:\n",
    "        idx = np.argmax(out)\n",
    "        tokens.append(unique[idx])\n",
    "    return tokens\n",
    "\n",
    "def even_embeddings(embed, n):\n",
    "    if len(embed) < n:\n",
    "        diff =  n - len(embed)\n",
    "        embed = np.vstack([embed, np.zeros((diff,len(embed[0])))])\n",
    "    return embed\n",
    "\n",
    "def even_tokens(tokens, n):\n",
    "    tokens = np.concatenate((['START'],tokens,['END'] ))\n",
    "    if len(tokens) < n:\n",
    "        diff =  n - len(tokens)\n",
    "        tokens = np.concatenate((tokens, np.full(diff, '')))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prime-bolivia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "init()\n",
    "unique = get_tokens()\n",
    "\n",
    "ds, _, test = read_naps_dataset()\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "Y_tokens = []\n",
    "Y_tokens2 = []\n",
    "max_embs_len = 0\n",
    "max_tokens_len = 0\n",
    "\n",
    "stopper = 1\n",
    "stopper2 = 1\n",
    "counter = 0\n",
    "\n",
    "with ds:\n",
    "    for d in ds:\n",
    "        if \"is_partial\" in d and d[\"is_partial\"]:\n",
    "            continue\n",
    "        counter += 1\n",
    "        masked_embs = get_embeddings(' '.join(d[\"text\"]))\n",
    "        if len(masked_embs) > max_embs_len:\n",
    "            max_embs_len = len(masked_embs)\n",
    "        X_train.append(masked_embs)    \n",
    "        tokens = make_the_tree_good(d[\"code_tree\"][\"funcs\"])\n",
    "        #print(make_the_tree_good(d[\"code_tree\"][\"funcs\"]))\n",
    "        if len(tokens) > max_tokens_len:\n",
    "            max_tokens_len = len(tokens)\n",
    "        Y_tokens.append(tokens)\n",
    "        #break\n",
    "        if counter == stopper:\n",
    "            break\n",
    "counter = 0\n",
    "with test:\n",
    "    for t in test:\n",
    "        if \"is_partial\" in t and t[\"is_partial\"]:\n",
    "            continue\n",
    "        counter += 1\n",
    "        masked_embs = get_embeddings(' '.join(t[\"text\"]))\n",
    "        if len(masked_embs) > max_embs_len:\n",
    "            max_embs_len = len(masked_embs)\n",
    "        X_test.append(masked_embs)    \n",
    "        tokens = make_the_tree_good(t[\"code_tree\"][\"funcs\"])\n",
    "        #print(make_the_tree_good(d[\"code_tree\"][\"funcs\"]))\n",
    "        if len(tokens) > max_tokens_len:\n",
    "            max_tokens_len = len(tokens)\n",
    "        Y_tokens2.append(tokens)\n",
    "        if counter == stopper2:\n",
    "            break\n",
    "        #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "criminal-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = []\n",
    "#print(X_train)\n",
    "for ex in X_train:\n",
    "    X_train_2.append(even_embeddings(ex, max_embs_len))\n",
    "\n",
    "for ex in Y_tokens:\n",
    "    #print(even_tokens(ex,max_tokens_len))\n",
    "    Y_train.append(generate_output(even_tokens(ex,max_tokens_len), unique))\n",
    "\n",
    "X_test_2 = []\n",
    "#print(X_train)\n",
    "for ex in X_test:\n",
    "    X_test_2.append(even_embeddings(ex, max_embs_len))\n",
    "\n",
    "for ex in Y_tokens2:\n",
    "    #print(even_tokens(ex,max_tokens_len))\n",
    "    Y_test.append(generate_output(even_tokens(ex,max_tokens_len), unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "behind-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = np.array(X_train_2)\n",
    "X_test_2 = np.array(X_test_2)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/X_train_2.npy', 'wb') as f:\n",
    "    np.save(f, X_train_2)\n",
    "    \n",
    "with open('data/X_test_2.npy', 'wb') as f:\n",
    "    np.save(f, X_test_2)\n",
    "    \n",
    "with open('Y_train.npy', 'wb') as f:\n",
    "    np.save(f, Y_train)\n",
    "    \n",
    "with open('Y_test.npy', 'wb') as f:\n",
    "    np.save(f, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-destiny",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
